{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!  git config --global user.email \"khayam.gondal@gmail.com\"\n",
    "!  git config --global user.name \"Khayam Gondal\"\n",
    "!  git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-10-13T23:00:14.083851Z",
     "iopub.status.busy": "2023-10-13T23:00:14.083333Z",
     "iopub.status.idle": "2023-10-13T23:00:56.633320Z",
     "shell.execute_reply": "2023-10-13T23:00:56.632768Z",
     "shell.execute_reply.started": "2023-10-13T23:00:14.083826Z"
    },
    "id": "JvMRbVLEJlZT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.14.1+cu117 requires torch==1.13.1, but you have torch 2.1.0 which is incompatible.\n",
      "torchtext 0.14.1 requires torch==1.13.1, but you have torch 2.1.0 which is incompatible.\n",
      "torchdata 0.5.1 requires torch==1.13.1, but you have torch 2.1.0 which is incompatible.\n",
      "torchaudio 0.13.1+cu117 requires torch==1.13.1, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -!\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#@title ðŸ¤— AutoTrain DreamBooth\n",
    "#@markdown In order to use this colab\n",
    "#@markdown - upload images to a folder named `images/`\n",
    "#@markdown - choose a project name if you wish\n",
    "#@markdown - change model if you wish, you can also select sd2/2.1 or sd1.5\n",
    "#@markdown - update prompt and remember it. choose keywords that don't usually appear in dictionaries\n",
    "#@markdown - add huggingface information (token and repo_id) if you wish to push trained model to huggingface hub\n",
    "#@markdown - update hyperparameters if you wish\n",
    "#@markdown - click `Runtime > Run all` or run each cell individually\n",
    "\n",
    "\n",
    "\n",
    "!pip install -U autotrain-advanced > install_logs.txt\n",
    "! pip install xformers git+https://github.com/huggingface/accelerate.git -q\n",
    "! pip install torch==2.1.0 torchvision==0.14.1+cu117 \\\n",
    " torchaudio==0.13.1+cu117 \\\n",
    " --index-url https://download.pytorch.org/whl/cu118 -q! pip install git+https://github.com/huggingface/diffusers -q\n",
    "! pip install git+https://github.com/huggingface/transformers -q\n",
    "! pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T23:00:57.719565Z",
     "iopub.status.busy": "2023-10-13T23:00:57.719328Z",
     "iopub.status.idle": "2023-10-13T23:00:58.827275Z",
     "shell.execute_reply": "2023-10-13T23:00:58.826475Z",
     "shell.execute_reply.started": "2023-10-13T23:00:57.719536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==2.1.0\n",
      "torchaudio==0.13.1+cu117\n",
      "torchdata==0.5.1\n",
      "torchtext==0.14.1\n",
      "torchvision==0.14.1+cu117\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T22:57:14.568285Z",
     "iopub.status.busy": "2023-10-13T22:57:14.567515Z",
     "iopub.status.idle": "2023-10-13T22:57:14.574924Z",
     "shell.execute_reply": "2023-10-13T22:57:14.574115Z",
     "shell.execute_reply.started": "2023-10-13T22:57:14.568260Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-10-13T22:49:12.766822Z",
     "iopub.status.busy": "2023-10-13T22:49:12.766254Z",
     "iopub.status.idle": "2023-10-13T22:49:12.772689Z",
     "shell.execute_reply": "2023-10-13T22:49:12.772126Z",
     "shell.execute_reply.started": "2023-10-13T22:49:12.766799Z"
    },
    "id": "A2-_lkBS1WKA"
   },
   "outputs": [],
   "source": [
    "#@markdown ---\n",
    "#@markdown #### Project Config\n",
    "project_name = 'my_dreambooth_project' # @param {type:\"string\"}\n",
    "model_name = 'stabilityai/stable-diffusion-xl-base-1.0' # @param [\"stabilityai/stable-diffusion-xl-base-1.0\", \"runwayml/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-2-1\", \"stabilityai/stable-diffusion-2-1-base\"]\n",
    "prompt = 'photo of a woman sitting on desk and working' # @param {type: \"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Push to Hub?\n",
    "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
    "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
    "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
    "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
    "push_to_hub = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "hf_token = \"hf_XXX\" #@param {type:\"string\"}\n",
    "repo_id = \"username/repo_name\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Hyperparameters\n",
    "learning_rate = 1e-4 # @param {type:\"number\"}\n",
    "num_steps = 500 #@param {type:\"number\"}\n",
    "batch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "gradient_accumulation = 4 # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "resolution = 1024 # @param {type:\"slider\", min:128, max:1024, step:128}\n",
    "use_8bit_adam = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "use_xformers = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "use_fp16 = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "train_text_encoder = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "gradient_checkpointing = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name\n",
    "os.environ[\"PROMPT\"] = prompt\n",
    "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"REPO_ID\"] = repo_id\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_STEPS\"] = str(num_steps)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"RESOLUTION\"] = str(resolution)\n",
    "os.environ[\"USE_8BIT_ADAM\"] = str(use_8bit_adam)\n",
    "os.environ[\"USE_XFORMERS\"] = str(use_xformers)\n",
    "os.environ[\"USE_FP16\"] = str(use_fp16)\n",
    "os.environ[\"TRAIN_TEXT_ENCODER\"] = str(train_text_encoder)\n",
    "os.environ[\"GRADIENT_CHECKPOINTING\"] = str(gradient_checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T23:02:54.967674Z",
     "iopub.status.busy": "2023-10-13T23:02:54.967123Z",
     "iopub.status.idle": "2023-10-13T23:36:44.597671Z",
     "shell.execute_reply": "2023-10-13T23:36:44.596791Z",
     "shell.execute_reply.started": "2023-10-13T23:02:54.967649Z"
    },
    "id": "g3cd_ED_yXXt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "> \u001b[1mINFO    Namespace(version=False, model='stabilityai/stable-diffusion-xl-base-1.0', revision=None, tokenizer=None, image_path='/datasets/man/', class_image_path=None, prompt='photo of a woman sitting on desk and working', class_prompt=None, num_class_images=100, class_labels_conditioning=None, prior_preservation=None, prior_loss_weight=1.0, project_name='my_dreambooth_project', seed=42, resolution=1024, center_crop=None, train_text_encoder=None, batch_size=1, sample_batch_size=4, epochs=1, num_steps=500, checkpointing_steps=100000, resume_from_checkpoint=None, gradient_accumulation=4, gradient_checkpointing=True, lr=0.0001, scale_lr=None, scheduler='constant', warmup_steps=0, num_cycles=1, lr_power=1.0, dataloader_num_workers=0, use_8bit_adam=True, adam_beta1=0.9, adam_beta2=0.999, adam_weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, allow_tf32=None, prior_generation_precision=None, local_rank=-1, xformers=True, pre_compute_text_embeddings=None, tokenizer_max_length=None, text_encoder_use_attention_mask=None, rank=4, xl=None, fp16=True, bf16=None, token=None, repo_id=None, push_to_hub=None, validation_prompt=None, num_validation_images=4, validation_epochs=50, checkpoints_total_limit=None, validation_images=None, logging=None, username=None, func=<function run_dreambooth_command_factory at 0x7f12f5bd39d0>)\u001b[0m\n",
      "> \u001b[1mINFO    Running DreamBooth Training\u001b[0m\n",
      "> \u001b[33m\u001b[1mWARNING Parameters supplied but not used: version, func\u001b[0m\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "{'variance_type', 'dynamic_thresholding_ratio', 'clip_sample_range', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "> \u001b[1mINFO    Enabling xformers\u001b[0m\n",
      "> \u001b[1mINFO    Enabling gradient checkpointing.\u001b[0m\n",
      "> \u001b[1mINFO    Computing text embeddings for prompt: photo of a woman sitting on desk and working\u001b[0m\n",
      "> \u001b[1mINFO    ***** Running training *****\u001b[0m\n",
      "> \u001b[1mINFO      Num examples = 23\u001b[0m\n",
      "> \u001b[1mINFO      Num batches each epoch = 23\u001b[0m\n",
      "> \u001b[1mINFO      Num Epochs = 84\u001b[0m\n",
      "> \u001b[1mINFO      Instantaneous batch size per device = 1\u001b[0m\n",
      "> \u001b[1mINFO      Total train batch size (w. parallel, distributed & accumulation) = 4\u001b[0m\n",
      "> \u001b[1mINFO      Gradient Accumulation steps = 4\u001b[0m\n",
      "> \u001b[1mINFO      Total optimization steps = 500\u001b[0m\n",
      "> \u001b[1mINFO      Training config = {'model': 'stabilityai/stable-diffusion-xl-base-1.0', 'revision': None, 'tokenizer': None, 'image_path': '/datasets/man/', 'class_image_path': None, 'prompt': 'photo of a woman sitting on desk and working', 'class_prompt': None, 'num_class_images': 100, 'class_labels_conditioning': None, 'prior_preservation': False, 'prior_loss_weight': 1.0, 'project_name': 'my_dreambooth_project', 'seed': 42, 'resolution': 1024, 'center_crop': False, 'train_text_encoder': False, 'batch_size': 1, 'sample_batch_size': 4, 'epochs': 84, 'num_steps': 500, 'checkpointing_steps': 100000, 'resume_from_checkpoint': None, 'gradient_accumulation': 4, 'gradient_checkpointing': True, 'lr': 0.0001, 'scale_lr': False, 'scheduler': 'constant', 'warmup_steps': 0, 'num_cycles': 1, 'lr_power': 1.0, 'dataloader_num_workers': 0, 'use_8bit_adam': True, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'allow_tf32': False, 'prior_generation_precision': None, 'local_rank': -1, 'xformers': True, 'pre_compute_text_embeddings': False, 'tokenizer_max_length': None, 'text_encoder_use_attention_mask': False, 'rank': 4, 'xl': True, 'fp16': True, 'bf16': False, 'token': None, 'repo_id': None, 'push_to_hub': False, 'username': None, 'validation_prompt': None, 'num_validation_images': 4, 'validation_epochs': 50, 'checkpoints_total_limit': None, 'validation_images': None, 'logging': False}\u001b[0m\n",
      "Steps:   0%|                                            | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/diffusers/models/attention_processor.py:1581: FutureWarning: `LoRAAttnProcessor2_0` is deprecated and will be removed in version 0.26.0. Make sure use AttnProcessor2_0 instead by settingLoRA layers to `self.{to_q,to_k,to_v,to_out[0]}.lora_layer` respectively. This will be done automatically when using `LoraLoaderMixin.load_lora_weights`\n",
      "  deprecate(\n",
      "Steps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [33:29<00:00,  3.90s/it, loss=0.175, lr=0.0001]Model weights saved in my_dreambooth_project/pytorch_lora_weights.safetensors\n",
      "Steps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [33:30<00:00,  4.02s/it, loss=0.175, lr=0.0001]\n",
      "rm: cannot remove '/datasets/man/camera_0.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_1.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_10.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_11.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_12.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_13.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_14.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_15.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_16.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_17.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_18.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_19.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_2.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_20.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_21.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_22.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_3.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_4.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_5.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_6.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_7.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_8.jpg': Read-only file system\n",
      "rm: cannot remove '/datasets/man/camera_9.jpg': Read-only file system\n"
     ]
    }
   ],
   "source": [
    "!autotrain dreambooth \\\n",
    "--model ${MODEL_NAME} \\\n",
    "--project-name ${PROJECT_NAME} \\\n",
    "--image-path /datasets/man/ \\\n",
    "--prompt \"${PROMPT}\" \\\n",
    "--resolution ${RESOLUTION} \\\n",
    "--batch-size ${BATCH_SIZE} \\\n",
    "--num-steps ${NUM_STEPS} \\\n",
    "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "--lr ${LEARNING_RATE} \\\n",
    "$( [[ \"$USE_FP16\" == \"True\" ]] && echo \"--fp16\" ) \\\n",
    "$( [[ \"$USE_XFORMERS\" == \"True\" ]] && echo \"--xformers\" ) \\\n",
    "$( [[ \"$TRAIN_TEXT_ENCODER\" == \"True\" ]] && echo \"--train-text-encoder\" ) \\\n",
    "$( [[ \"$USE_8BIT_ADAM\" == \"True\" ]] && echo \"--use-8bit-adam\" ) \\\n",
    "$( [[ \"$GRADIENT_CHECKPOINTING\" == \"True\" ]] && echo \"--gradient-checkpointing\" ) \\\n",
    "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T00:21:57.492864Z",
     "iopub.status.busy": "2023-10-14T00:21:57.492078Z",
     "iopub.status.idle": "2023-10-14T00:22:24.450295Z",
     "shell.execute_reply": "2023-10-14T00:22:24.449668Z",
     "shell.execute_reply.started": "2023-10-14T00:21:57.492830Z"
    },
    "id": "_LvIS7-7PcLT"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48110acbaa14034b50297819349c8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d50bb9184d3440cbdc90af81204dd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionXLImg2ImgPipeline {\n",
       "  \"_class_name\": \"StableDiffusionXLImg2ImgPipeline\",\n",
       "  \"_diffusers_version\": \"0.22.0.dev0\",\n",
       "  \"_name_or_path\": \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
       "  \"force_zeros_for_empty_prompt\": false,\n",
       "  \"requires_aesthetics_score\": true,\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"text_encoder_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModelWithProjection\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"tokenizer_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "# this is the inference code that you can use after you have trained your model\n",
    "# Unhide code below and change prj_path to your repo or local path (e.g. my_dreambooth_project)\n",
    "#\n",
    "#\n",
    "#\n",
    "from diffusers import DiffusionPipeline, StableDiffusionXLImg2ImgPipeline\n",
    "import torch\n",
    "\n",
    "prj_path = \"my_dreambooth_project\"\n",
    "model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "     model,\n",
    "     torch_dtype=torch.float16,\n",
    " )\n",
    "pipe.to(\"cuda\")\n",
    "pipe.load_lora_weights(prj_path, weight_name=\"pytorch_lora_weights.safetensors\")\n",
    "\n",
    "refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "     \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "     torch_dtype=torch.float16,\n",
    ")\n",
    "refiner.to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T00:23:27.540697Z",
     "iopub.status.busy": "2023-10-14T00:23:27.540423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d506a8bcc4e40e5bf58d20663363f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"woman with with lana del ray face sitting in the office\"\n",
    "\n",
    "seed = 42\n",
    "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "image = pipe(prompt=prompt, generator=generator).images[0]\n",
    "image = refiner(prompt=prompt, generator=generator, image=image).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T23:45:58.988854Z",
     "iopub.status.busy": "2023-10-13T23:45:58.988360Z",
     "iopub.status.idle": "2023-10-13T23:46:02.567232Z",
     "shell.execute_reply": "2023-10-13T23:46:02.566584Z",
     "shell.execute_reply.started": "2023-10-13T23:45:58.988830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.*\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.4\n",
      "    Uninstalling protobuf-4.23.4:\n",
      "      Successfully uninstalled protobuf-4.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow 2.9.2 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.14.1 which is incompatible.\n",
      "autotrain-advanced 0.6.36 requires protobuf==4.23.4, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T00:21:45.436518Z",
     "iopub.status.busy": "2023-10-14T00:21:45.436076Z",
     "iopub.status.idle": "2023-10-14T00:21:49.510454Z",
     "shell.execute_reply": "2023-10-14T00:21:49.509676Z",
     "shell.execute_reply.started": "2023-10-14T00:21:45.436493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 09b6eb3] updated notebook via paperspace\n",
      " 2 files changed, 477 insertions(+), 131 deletions(-)\n",
      "Enumerating objects: 7, done.\n",
      "Counting objects: 100% (7/7), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 1.64 MiB | 7.63 MiB/s, done.\n",
      "Total 4 (delta 1), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
      "To https://github.com/khayamgondal/diffusers.git\n",
      "   4bb85ad..09b6eb3  main -> main\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31m.gradient/\u001b[m\n",
      "\t\u001b[31m.ipynb_checkpoints/\u001b[m\n",
      "\t\u001b[31mgenerated_image.png\u001b[m\n",
      "\t\u001b[31minstall_logs.txt\u001b[m\n",
      "\t\u001b[31mmy_dreambooth_project/\u001b[m\n",
      "\t\u001b[31msetup_logs.txt\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "! git commit -a -m \"updated notebook via paperspace\"\n",
    "! git push\n",
    "! git status"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
