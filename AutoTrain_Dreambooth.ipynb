{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-10-13T22:40:21.843531Z",
     "iopub.status.busy": "2023-10-13T22:40:21.842809Z",
     "iopub.status.idle": "2023-10-13T22:41:08.956611Z",
     "shell.execute_reply": "2023-10-13T22:41:08.956051Z",
     "shell.execute_reply.started": "2023-10-13T22:40:21.843504Z"
    },
    "id": "JvMRbVLEJlZT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/diffusers/utils/import_utils.py\", line 684, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py\", line 20, in <module>\n",
      "    from transformers import CLIPTextModel, CLIPTextModelWithProjection, CLIPTokenizer\n",
      "ImportError: cannot import name 'CLIPTextModelWithProjection' from 'transformers' (/usr/local/lib/python3.9/dist-packages/transformers/__init__.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/autotrain\", line 5, in <module>\n",
      "    from autotrain.cli.autotrain import main\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/autotrain/cli/autotrain.py\", line 6, in <module>\n",
      "    from .run_dreambooth import RunAutoTrainDreamboothCommand\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/autotrain/cli/run_dreambooth.py\", line 10, in <module>\n",
      "    from autotrain.trainers.dreambooth.__main__ import train as train_dreambooth\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/autotrain/trainers/dreambooth/__main__.py\", line 11, in <module>\n",
      "    from diffusers import StableDiffusionXLPipeline\n",
      "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/diffusers/utils/import_utils.py\", line 675, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/diffusers/utils/import_utils.py\", line 675, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/diffusers/utils/import_utils.py\", line 674, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/diffusers/utils/import_utils.py\", line 686, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl because of the following error (look up to see its traceback):\n",
      "cannot import name 'CLIPTextModelWithProjection' from 'transformers' (/usr/local/lib/python3.9/dist-packages/transformers/__init__.py)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#@title ðŸ¤— AutoTrain DreamBooth\n",
    "#@markdown In order to use this colab\n",
    "#@markdown - upload images to a folder named `images/`\n",
    "#@markdown - choose a project name if you wish\n",
    "#@markdown - change model if you wish, you can also select sd2/2.1 or sd1.5\n",
    "#@markdown - update prompt and remember it. choose keywords that don't usually appear in dictionaries\n",
    "#@markdown - add huggingface information (token and repo_id) if you wish to push trained model to huggingface hub\n",
    "#@markdown - update hyperparameters if you wish\n",
    "#@markdown - click `Runtime > Run all` or run each cell individually\n",
    "\n",
    "import os\n",
    "\n",
    "!pip install -U autotrain-advanced > install_logs.txt\n",
    "! pip install xformers git+https://github.com/huggingface/accelerate.git -q\n",
    "! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "! pip install git+https://github.com/huggingface/diffusers -q\n",
    "! pip install git+https://github.com/huggingface/transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T22:41:27.550820Z",
     "iopub.status.busy": "2023-10-13T22:41:27.550498Z",
     "iopub.status.idle": "2023-10-13T22:41:28.598685Z",
     "shell.execute_reply": "2023-10-13T22:41:28.598066Z",
     "shell.execute_reply.started": "2023-10-13T22:41:27.550812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers==2.2.2\n",
      "transformers @ git+https://github.com/huggingface/transformers@21dc5859421cf0d7d82d374b10f533611745a8c5\n"
     ]
    }
   ],
   "source": [
    "! pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-10-13T22:38:21.456276Z",
     "iopub.status.busy": "2023-10-13T22:38:21.456111Z",
     "iopub.status.idle": "2023-10-13T22:38:21.464911Z",
     "shell.execute_reply": "2023-10-13T22:38:21.464360Z",
     "shell.execute_reply.started": "2023-10-13T22:38:21.456256Z"
    },
    "id": "A2-_lkBS1WKA"
   },
   "outputs": [],
   "source": [
    "#@markdown ---\n",
    "#@markdown #### Project Config\n",
    "project_name = 'my_dreambooth_project' # @param {type:\"string\"}\n",
    "model_name = 'stabilityai/stable-diffusion-xl-base-1.0' # @param [\"stabilityai/stable-diffusion-xl-base-1.0\", \"runwayml/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-2-1\", \"stabilityai/stable-diffusion-2-1-base\"]\n",
    "prompt = 'photo of a woman sitting on desk and working' # @param {type: \"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Push to Hub?\n",
    "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
    "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
    "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
    "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
    "push_to_hub = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "hf_token = \"hf_XXX\" #@param {type:\"string\"}\n",
    "repo_id = \"username/repo_name\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Hyperparameters\n",
    "learning_rate = 1e-4 # @param {type:\"number\"}\n",
    "num_steps = 500 #@param {type:\"number\"}\n",
    "batch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "gradient_accumulation = 4 # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "resolution = 1024 # @param {type:\"slider\", min:128, max:1024, step:128}\n",
    "use_8bit_adam = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "use_xformers = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "use_fp16 = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "train_text_encoder = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "gradient_checkpointing = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name\n",
    "os.environ[\"PROMPT\"] = prompt\n",
    "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"REPO_ID\"] = repo_id\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_STEPS\"] = str(num_steps)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"RESOLUTION\"] = str(resolution)\n",
    "os.environ[\"USE_8BIT_ADAM\"] = str(use_8bit_adam)\n",
    "os.environ[\"USE_XFORMERS\"] = str(use_xformers)\n",
    "os.environ[\"USE_FP16\"] = str(use_fp16)\n",
    "os.environ[\"TRAIN_TEXT_ENCODER\"] = str(train_text_encoder)\n",
    "os.environ[\"GRADIENT_CHECKPOINTING\"] = str(gradient_checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T22:41:33.522010Z",
     "iopub.status.busy": "2023-10-13T22:41:33.521723Z"
    },
    "id": "g3cd_ED_yXXt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "> \u001b[1mINFO    Namespace(version=False, model='stabilityai/stable-diffusion-xl-base-1.0', revision=None, tokenizer=None, image_path='/datasets/man/', class_image_path=None, prompt='photo of a woman sitting on desk and working', class_prompt=None, num_class_images=100, class_labels_conditioning=None, prior_preservation=None, prior_loss_weight=1.0, project_name='my_dreambooth_project', seed=42, resolution=1024, center_crop=None, train_text_encoder=None, batch_size=1, sample_batch_size=4, epochs=1, num_steps=500, checkpointing_steps=100000, resume_from_checkpoint=None, gradient_accumulation=4, gradient_checkpointing=True, lr=0.0001, scale_lr=None, scheduler='constant', warmup_steps=0, num_cycles=1, lr_power=1.0, dataloader_num_workers=0, use_8bit_adam=True, adam_beta1=0.9, adam_beta2=0.999, adam_weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, allow_tf32=None, prior_generation_precision=None, local_rank=-1, xformers=True, pre_compute_text_embeddings=None, tokenizer_max_length=None, text_encoder_use_attention_mask=None, rank=4, xl=None, fp16=True, bf16=None, token=None, repo_id=None, push_to_hub=None, validation_prompt=None, num_validation_images=4, validation_epochs=50, checkpoints_total_limit=None, validation_images=None, logging=None, username=None, func=<function run_dreambooth_command_factory at 0x7f5d4784a040>)\u001b[0m\n",
      "> \u001b[1mINFO    Running DreamBooth Training\u001b[0m\n",
      "> \u001b[33m\u001b[1mWARNING Parameters supplied but not used: version, func\u001b[0m\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 737/737 [00:00<00:00, 130kB/s]\n",
      "Downloading (â€¦)tokenizer/vocab.json: 100%|â–ˆ| 1.06M/1.06M [00:00<00:00, 34.6MB/s]\n",
      "Downloading (â€¦)tokenizer/merges.txt: 100%|â–ˆâ–ˆâ–ˆ| 525k/525k [00:00<00:00, 38.1MB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 472/472 [00:00<00:00, 398kB/s]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 725/725 [00:00<00:00, 228kB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460/460 [00:00<00:00, 391kB/s]\n",
      "Downloading (â€¦)_encoder/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:00<00:00, 175kB/s]\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "Downloading (â€¦)ncoder_2/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 575/575 [00:00<00:00, 484kB/s]\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 492M/492M [00:11<00:00, 44.3MB/s]\n",
      "Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.78G/2.78G [01:04<00:00, 43.1MB/s]\n",
      "Downloading (â€¦)main/vae/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 642/642 [00:00<00:00, 125kB/s]\n",
      "Downloading (â€¦)ch_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 335M/335M [00:02<00:00, 165MB/s]\n",
      "Downloading (â€¦)ain/unet/config.json: 100%|â–ˆâ–ˆ| 1.68k/1.68k [00:00<00:00, 338kB/s]\n",
      "Downloading (â€¦)ch_model.safetensors:  60%|â–Œ| 6.18G/10.3G [00:51<01:54, 35.8MB/s]"
     ]
    }
   ],
   "source": [
    "!autotrain dreambooth \\\n",
    "--model ${MODEL_NAME} \\\n",
    "--project-name ${PROJECT_NAME} \\\n",
    "--image-path /datasets/man/ \\\n",
    "--prompt \"${PROMPT}\" \\\n",
    "--resolution ${RESOLUTION} \\\n",
    "--batch-size ${BATCH_SIZE} \\\n",
    "--num-steps ${NUM_STEPS} \\\n",
    "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "--lr ${LEARNING_RATE} \\\n",
    "$( [[ \"$USE_FP16\" == \"True\" ]] && echo \"--fp16\" ) \\\n",
    "$( [[ \"$USE_XFORMERS\" == \"True\" ]] && echo \"--xformers\" ) \\\n",
    "$( [[ \"$TRAIN_TEXT_ENCODER\" == \"True\" ]] && echo \"--train-text-encoder\" ) \\\n",
    "$( [[ \"$USE_8BIT_ADAM\" == \"True\" ]] && echo \"--use-8bit-adam\" ) \\\n",
    "$( [[ \"$GRADIENT_CHECKPOINTING\" == \"True\" ]] && echo \"--gradient-checkpointing\" ) \\\n",
    "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LvIS7-7PcLT"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "# this is the inference code that you can use after you have trained your model\n",
    "# Unhide code below and change prj_path to your repo or local path (e.g. my_dreambooth_project)\n",
    "#\n",
    "#\n",
    "#\n",
    "# from diffusers import DiffusionPipeline, StableDiffusionXLImg2ImgPipeline\n",
    "# import torch\n",
    "\n",
    "# prj_path = \"username/repo_name\"\n",
    "# model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "# pipe = DiffusionPipeline.from_pretrained(\n",
    "#     model,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "# pipe.to(\"cuda\")\n",
    "# pipe.load_lora_weights(prj_path, weight_name=\"pytorch_lora_weights.safetensors\")\n",
    "\n",
    "# refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "#     \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "# refiner.to(\"cuda\")\n",
    "\n",
    "# prompt = \"photo of a sks dog in a bucket\"\n",
    "\n",
    "# seed = 42\n",
    "# generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "# image = pipe(prompt=prompt, generator=generator).images[0]\n",
    "# image = refiner(prompt=prompt, generator=generator, image=image).images[0]\n",
    "# image.save(f\"generated_image.png\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
